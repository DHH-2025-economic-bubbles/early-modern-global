{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedcec70",
   "metadata": {},
   "source": [
    "# TF-IDF notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88051c",
   "metadata": {},
   "source": [
    "# Cleaning data \n",
    "In order to run tf-idf, we need to do two things:\n",
    "- remove punctuation from text\n",
    "- combine recognizeable place names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d452ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 483/232695\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m output_file = CLEANED_DATA_FOLDER / json_file.name\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     24\u001b[39m     json.dump(file_result, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:186\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from src.settings import FOLDER_ARTICLES, DATA_FOLDER\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import json\n",
    "\n",
    "CLEANED_DATA_FOLDER = DATA_FOLDER / \"cleaned_json\"\n",
    "CLEANED_DATA_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "json_files = list(FOLDER_ARTICLES.glob(\"*.json\"))\n",
    "\n",
    "cleaned_articles = []\n",
    "counter = 0\n",
    "total_files = len(json_files)\n",
    "from tfidf_imports import clean_text  # must be imported from another file in order for multiprocessing to work\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for json_file, file_result in zip(json_files, executor.map(clean_text, json_files)):\n",
    "        cleaned_articles.append(file_result)\n",
    "        counter += 1\n",
    "        print(f\"Processing file {counter}/{total_files}\", end=\"\\r\")\n",
    "\n",
    "        output_file = CLEANED_DATA_FOLDER / json_file.name\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(file_result, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7352d",
   "metadata": {},
   "source": [
    "# Tokenize place names\n",
    "We have a list of place names which occasionally have spaces and such. We need to process it such that you capitalize them, and concatenate them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d08c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLONIAL_PLACES_UNPROCESSED = ['darma pass', 'tawang', 'dirang', 'tulung la', 'nyuri', 'bomdi la', 'lhau', 'pangchen', 'lumpo', 'chigup la', 'jang', 'foot hills camp', 'se la', 'jamiri', \n",
    "                   'lha la', 'balipara', 'kashong la', 'sarai amanat khan', 'patiala', 'jalandhar', 'fatehgarh', 'bir kishansingh', 'hayatnagar', 'jaunpur', 'cossimbazar', 'bhagalpur', \n",
    "                   'sebu la', 'baramula', 'lachen', 'tezpur', 'bomdila', 'quilon', 'essau', 'benin-city', 'pategi', 'khajuraho', 'calliena', 'tangtse', 'champai', 'dilkawn', 'haulawng', \n",
    "                   'hlaikhan', 'hydel rest house', 'kacharian forest rest house', 'lumtui', 'mogilipenta', 'nagar hole', 'neokka rest house', 'pahara puri rest house', 'patainag rest house', \n",
    "                   'pili ki khan', 'sita bani', 'tapuraki khan', 'agham', 'chagra', 'chalan chumik', 'chartse', 'chhong jangal', 'chongdongmale', 'chung tung', 'dachung yogma', 'dakbajan', \n",
    "                   'dangangongmale', 'dangyailak', 'daulat beg oldi', 'debring', 'fariabad', 'gangrale', 'gapshan', 'goma tharu', 'hurling', 'kaksang', 'kanatal', 'karale', 'kataklik', 'katlang chenmo', \n",
    "                   'kenlung', 'khala', 'khargok fort', 'kharlung', 'khiangshisa', 'khurmafu', 'khurna sumdo', 'lakong', 'lama guru', 'lapurba', 'lashitanga', 'lekaru', 'lichu', 'lingti', 'loma', 'lungkung', \n",
    "                   'lungnakle', 'lungturma', 'luntunnu', 'mankhang', 'karur', 'miksadiu', 'muldem', 'mundio', 'murshun', 'nelda serbonle', 'niri', 'niri sumdo', 'numah', 'nyamur', 'orarucha', 'pachatang', \n",
    "                   'pamzal', 'pang', 'pangot', 'patseo', 'peldo', 'pololung', 'pungiado', 'purang sumdo', 'puti runi', 'rachogba', 'ramjak', 'rimdi', 'sangcha talla', 'sangtha', 'saser brangsa', \n",
    "                   'shokpa kunglang', 'shurok sumdo', 'spanggur', 'sumdo', 'sumdo (2)', 'sumdole', 'sumdole (2)', 'surai thota', 'sutak', 'tangoleb', 'tangyar', 'tarakokpole', 'topo koma', 'trakkur', \n",
    "                   'tsakshang', 'tsogstsolu', 'tut yailak', 'umdung', 'umlung', 'yaglung', 'yahle', 'yargulak', 'zingzingbar', 'katra', 'karvan', 'karvan', 'katra (2)', 'katra (3)', 'katra (4)', 'katra (5)',\n",
    "                'katra (6)', 'khatra (6)', 'kodungallur', 'paithan', 'lothal', 'sopara', 'kalyan', 'simylla', 'pondicherry', 'tamluk', 'kaverippattanam', 'korkai', 'sholapur', 'kargil', 'bourem', 'bahadurpur', \n",
    "                    'shahabad', 'borghat pass', 'nanaghat', 'thal ghat', 'mathura', 'mathura (2)', 'srinagar', 'ujjain', 'yola', 'bauchi', 'zaria', 'katsina', 'sokoto', 'gwandu', 'birnin-kebbi', 'salaga', \n",
    "                    'waypoint-2375', 'cuttack', 'mangalore', 'chennai', 'kolkata', 'sasaram', 'darjiling', 'chekawn', 'gongrale', 'hodal', 'allahabad', 'patna', 'agra', 'hyderabad (2)', 'jaipur', 'ahmadabad', \n",
    "                    'ahmadnagar', 'ajmer', 'vadodara', 'beawar', 'gogunda', 'pali', 'ambala', 'pir panjal pass', 'handia', 'aurangabad', 'baleshwar', 'bayana', 'bijapur', 'varanasi', 'burhanpur', 'khambhat', \n",
    "                    'chopda', 'delhi', 'dhaulpur', 'gwalior', 'jodhpur', 'karnal', 'kolaras', 'ludhiana', 'mandu', 'mehsana', 'nandurbar', 'panipat', 'pipar', 'rajmahal', 'sehore', 'fatehpur sikri', 'sirhind', \n",
    "                    'sironj', 'surat', 'udaipur', 'narwar', 'sonipat', 'asirgarh', 'sirohi', 'shahabad', 'kano', 'abuja', 'acadia', 'accra', 'acton', 'adobe walls', 'agra', 'agra', 'ahmedabad', 'ajmer', 'ajmer', \n",
    "                    'akbarpur', 'alabama', 'alabama', 'alaska', 'alaska range', 'alberta', 'aleutian islands', 'alexandria', 'allahabad', 'allahabad', 'alwar', 'amritsar', 'andaman islands', 'anjengo', \n",
    "                    'appalachian mountains', 'arapaho', 'arctic ocean', 'arizona', 'arkansas', 'arkansas', 'arkat', 'asante', 'assam', 'atlan', 'athabasca', 'atlanta', 'austin', 'awadh', 'axim', 'ayodhya', \n",
    "                    'baffin island', 'bahamas', 'bajwara', 'bangala', 'balasore', 'baltimore', 'banks island', 'barar', 'barbados', 'baroda', 'baton rouge', 'bauchi', 'bayana', 'beas', 'belize', 'bemis heights', 'benares', \n",
    "                    'bengal', 'bennington', 'benue', 'berbice', 'bhatkal', 'bhopal', 'bhopal', 'bida', 'bidar', 'bidar', 'big cypress swamp', 'big meadows', 'bihar', 'bijapur', 'biloxi', 'bimlipatam', 'bird creek', 'blackstock', \n",
    "                    'boise', 'bono', 'boston', 'brahmaputra', 'brazos', 'british columbia', 'british guiana', 'british honduras', 'british north borneo', 'brooks range', 'burhanpur', 'cachar', 'cahokia', 'calabar', 'calcutta', 'calgary', \n",
    "                    'calicut', 'california', 'cambay', 'camden', 'canada', 'canyon de chelly', 'cape breton island', 'carrier', 'carson city', 'cascades', 'cawnpore', 'cayes', 'central india agency', 'central provinces', 'ceylon', 'chambal', \n",
    "                     'chandernagore', 'charlotte', 'charlottesville', 'chaul', 'chicago', 'chiaha', 'chinsura', 'chitimacha', 'chitor', 'cincinnati', 'cleveland', 'coast mountains', 'colombo', 'colorado', 'colorado', 'connecticut', 'connecticut', 'coondapoor', \n",
    "                     'coorg', 'coromandel coast', 'cossimbazar', 'costa rica', 'cranganur', 'culver city', 'cumberland river', 'cuttack', 'dallas', 'daman', 'deccan', 'deccan states', 'delaware', 'delaware', 'delhi', 'demerara', 'denver', \n",
    "                     'des moines', 'detroit', 'dharangaon', 'dikwa', 'dominica', 'dove creek', 'eastmain', 'eastern ghats', 'edmonton', 'el paso', 'ellesmere island', 'ellora', 'elmina', 'erie', 'erie, lake', 'etawah', 'fallen timbers', 'flathead', \n",
    "                     'florida', 'fort beauséjour', 'fort chambly', 'fort dearborn', 'fort detroit', 'fort duquesne', 'fort frontenac', 'fort kearney', 'fort liberté', 'fort malden', 'fort mellon', 'fort mims', 'fort necessity', 'fort niagara', 'fort orange', \n",
    "                     'fort oswego', 'fort stanwix', 'fort ticonderoga', 'fort william', 'fowltown', 'frankfort', 'frederiksnagar', \"freeman's farm\", 'fyzabad', 'ganges', 'garhgaon', 'georgia', 'georgian bay', 'ghaghara', 'gobir', 'godavari', 'goliad massacre',\n",
    "                       'golkonda', 'golkonda', 'gombe', 'gonave', 'gooty', 'granada', 'grand canyon', 'great bear lake', 'great plains', 'great salt lake', 'great slave lake', 'guiana', 'gujarat', 'gulbarga', 'gwalior', 'gwandu', 'hadejia', 'haiti', 'halifax', \n",
    "                       'harrisburg', 'harrodsborough', 'hartford', 'havasupai', 'hawaiian islands', 'honavar', 'honduras', 'honolulu', 'hooghly', 'houma', 'houston', 'howrah', 'hualapai', 'hubli', 'huron, lake', 'hyderabad', 'hyderabad', 'idaho', 'ijebu ode', \n",
    "                'illichpur', 'illinois', 'illinois', 'ilorin', 'imphal', 'india', 'indiana', 'indianapolis', 'indore', 'iowa', 'jacksonville', 'jaipur', 'jalalpur', 'jamaica', 'jammu and kashmir', 'jamshedpur', 'jefferson city', 'jharkhand', 'jicarilla apache',\n",
    "                'johore', 'junagarh', 'junnar', 'kaabu', 'kalhat', 'kanara', 'kandy', 'kano', 'kano', 'kansas', 'kansas city', 'karankawa', 'karikal', 'kashmir', 'kaskaskia', 'katagum', 'katak', 'kathiawar', 'katsina', 'katsina', 'kaveri', 'kayankulam', 'kedah', \n",
    "                 'kentucky', 'khairabad', 'khandesh', 'khurja', 'kiet siel', \"king's mountain\", 'kingston', 'knoxville', 'kolhapur', 'konkan', 'krishna', 'kuala lumpur', 'kukawa', 'kumaun', 'labrador', 'lakhawar', 'lansing', 'laurentian highlands', 'lipan', \n",
    "                 'little rock', 'long island', 'los angeles', 'louisbourg', 'louisiana', 'louisville', 'lucknow', 'luni', 'mackenzie', 'mackenzie mountains', 'madras', 'madura', 'madurai', 'mahanadi', 'mahe', 'maheshwar', 'maine', 'malabar coast', 'malacca', \n",
    "                 'manoa', 'malaya', 'malaysia', 'malwa', 'managua', 'mangalore', 'mangalur', 'manipur', 'manitoba', 'maryland', 'massachusetts', 'masulipatam', 'maui', 'mescalero', 'mescalero apache', 'miami', 'michigan', 'michigan, lake', 'milwaukee', 'minnesota',                   \n",
    "                 'mississippi', 'mississippi', 'missouri', 'missouri', 'mobile', 'mojave', 'molokai', 'montana', 'montreal', 'mosquito coast', 'mubi', 'murshidabad', 'mysore', 'mysore', 'nagpur', 'nashville', 'navajo', 'nebraska', 'nefertare', 'negombo', 'nevada', \n",
    "                 'new amsterdam', 'new brunswick', 'new calabar', 'new england', 'new hampshire', 'new haven', 'new jersey', 'new mexico', 'new orleans', 'new ulm', 'new york', 'new york', 'newfoundland', 'newfoundland', 'niagara', 'nicaragua', 'niger', 'ningi', \n",
    "                 \"nizam's dominions\", 'nizampatam', 'north carolina', 'north dakota', 'northwest territories', 'nova scotia', 'oahu', 'ohio', 'ohio', 'ojibwa', 'oklahoma', 'ontario', 'ontario, lake', 'orchha', 'oregon', 'orisa', 'orissa', 'ottawa', 'ottawa', 'ottawa', \n",
    "                'oudh', 'paiute', 'panamá', 'panipat', 'passamaquoddy', 'pedee', 'pee dee', 'penang', 'pennsylvania', 'pensacola', 'pensacola', 'pequot', 'perak', 'philadelphia', 'plassey', 'poona', 'pondicherry', 'port-au-prince', 'portobelo', 'prince edward island',\n",
    "                'puttalam', 'quapaw', 'quebec', 'quebec', 'quilon', 'rajputana', 'raleigh', 'rampur', 'regina', 'reindeer lake', 'rhode island', 'roanoke', 'rohilkhand', 'rupert house', 'sabarmati', 'sacramento', 'sadras', 'salt lake city', 'samana', 'san antonio', \n",
    "                'san diego', 'san francisco', 'san josé', 'santa fe trail', 'sarawak', 'sarkhej', 'saskatchewan', 'saskatchewan', 'saskatoon', 'satpura range', 'savanur', 'seattle', 'serampore', 'seringapatam', 'shillong', 'sholapur', 'shorapur', 'shreveport',\n",
    "                'sindhia', 'sira', 'sironj', 'slave', 'soccorra', 'sokoto', 'sokoto', 'south carolina', 'south dakota', 'southampton island', 'srinagar', 'st louis', 'st lucia', 'stabroek', 'steptoe butte', 'sudbury', 'superior, lake', 'surat', 'taiping', \n",
    "                'tallahassee', 'tamathli', 'tanjore', 'tanjore', 'tegucigalpa', 'tehri garhwal', 'tellicherry', 'tennessee', 'tennessee', 'tewa', 'texas', 'thar desert', 'tipecanoe', 'tobago', 'tonkawa', 'topeka', 'toronto', 'trincomalee', 'trinidad', 'tripura', \n",
    "                'trivandrum', 'truckee', 'tucson', 'tuskegee', 'tuticorin', 'ujjain', 'undi', 'united states of america', 'utah', 'valley forge', 'vancouver', 'vengurla', 'vermont', 'vindhya range', 'virginia', 'vizagapatam', 'waccamaw', 'wandiwash', 'warangal', \n",
    "                'washington dc', 'west florida', 'west point', 'west virginia', 'western ghats', 'white volta', 'whitehorse', 'whitestone hill', 'wilkes-barre', 'winneba', 'winnipeg', 'wisconsin', 'wyoming', 'yamuna', 'yanam', 'yazoo', 'yellowknife', 'yellowstone', \n",
    "                'yola', 'yukon', 'yukon territory', 'zamfara', 'zaria', 'zaria', 'united states of america', 'trinidad and tobago', \n",
    "                'sri lanka', 'sierra leone', 'saint lucia', 'panama', 'nigeria', 'nicaragua', 'malaysia', 'jamaica', 'india', 'honduras', 'haiti', 'guyana', 'grenada', 'ghana', 'gambia', 'dominica', 'costa rica', 'canada', 'belize', 'barbados', 'bahamas']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3749a8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check: ['ABUJA', 'ACADIA', 'ACCRA', 'ACTON', 'ADOBEWALLS', 'AGHAM', 'AGRA', 'AHMADABAD', 'AHMADNAGAR', 'AHMEDABAD', 'AJMER', 'AKBARPUR', 'ALABAMA', 'ALASKA', 'ALASKARANGE', 'ALBERTA', 'ALEUTIANISLANDS', 'ALEXANDRIA', 'ALLAHABAD', 'ALWAR', 'AMBALA', 'AMRITSAR', 'ANDAMANISLANDS', 'ANJENGO', 'APPALACHIANMOUNTAINS', 'ARAPAHO', 'ARCTICOCEAN', 'ARIZONA', 'ARKANSAS', 'ARKAT', 'ASANTE', 'ASIRGARH', 'ASSAM', 'ATHABASCA', 'ATLAN', 'ATLANTA', 'AURANGABAD', 'AUSTIN', 'AWADH', 'AXIM', 'AYODHYA', 'BAFFINISLAND', 'BAHADURPUR', 'BAHAMAS', 'BAJWARA', 'BALASORE', 'BALESHWAR', 'BALIPARA', 'BALTIMORE', 'BANGALA', 'BANKSISLAND', 'BARAMULA', 'BARAR', 'BARBADOS', 'BARODA', 'BATONROUGE', 'BAUCHI', 'BAYANA', 'BEAS', 'BEAWAR', 'BELIZE', 'BEMISHEIGHTS', 'BENARES', 'BENGAL', 'BENINCITY', 'BENNINGTON', 'BENUE', 'BERBICE', 'BHAGALPUR', 'BHATKAL', 'BHOPAL', 'BIDA', 'BIDAR', 'BIGCYPRESSSWAMP', 'BIGMEADOWS', 'BIHAR', 'BIJAPUR', 'BILOXI', 'BIMLIPATAM', 'BIRDCREEK', 'BIRKISHANSINGH', 'BIRNINKEBBI', 'BLACKSTOCK', 'BOISE', 'BOMDILA', 'BONO', 'BORGHATPASS', 'BOSTON', 'BOUREM', 'BRAHMAPUTRA', 'BRAZOS', 'BRITISHCOLUMBIA', 'BRITISHGUIANA', 'BRITISHHONDURAS', 'BRITISHNORTHBORNEO', 'BROOKSRANGE', 'BURHANPUR', 'CACHAR', 'CAHOKIA', 'CALABAR']\n",
      "sanity check: ['abuja', 'acadia', 'accra', 'acton', 'adobe walls', 'agham', 'agra', 'ahmadabad', 'ahmadnagar', 'ahmedabad', 'ajmer', 'akbarpur', 'alabama', 'alaska', 'alaska range', 'alberta', 'aleutian islands', 'alexandria', 'allahabad', 'alwar', 'ambala', 'amritsar', 'andaman islands', 'anjengo', 'appalachian mountains', 'arapaho', 'arctic ocean', 'arizona', 'arkansas', 'arkat', 'asante', 'asirgarh', 'assam', 'athabasca', 'atlan', 'atlanta', 'aurangabad', 'austin', 'awadh', 'axim', 'ayodhya', 'baffin island', 'bahadurpur', 'bahamas', 'bajwara', 'balasore', 'baleshwar', 'balipara', 'baltimore', 'bangala', 'banks island', 'baramula', 'barar', 'barbados', 'baroda', 'baton rouge', 'bauchi', 'bayana', 'beas', 'beawar', 'belize', 'bemis heights', 'benares', 'bengal', 'benincity', 'bennington', 'benue', 'berbice', 'bhagalpur', 'bhatkal', 'bhopal', 'bida', 'bidar', 'big cypress swamp', 'big meadows', 'bihar', 'bijapur', 'biloxi', 'bimlipatam', 'bir kishansingh', 'bird creek', 'birninkebbi', 'blackstock', 'boise', 'bomdi la', 'bomdila', 'bono', 'borghat pass', 'boston', 'bourem', 'brahmaputra', 'brazos', 'british columbia', 'british guiana', 'british honduras', 'british north borneo', 'brooks range', 'burhanpur', 'cachar', 'cahokia']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_regex(unfixed_string: str): \n",
    "    return re.sub(r\"\\s*\\(\\d+\\)\", \"\", unfixed_string)\n",
    "\n",
    "def clean_punct(punctuated_string: str):\n",
    "    return punctuated_string.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "\n",
    "def capitalize_and_concat(unfixed_string: str) -> str:\n",
    "    # get rid of \" (number)\" suffices with regex\n",
    "    no_regex = clean_regex(unfixed_string)\n",
    "\n",
    "    # get rid of spaces\n",
    "    no_spaces = no_regex.replace(\" \", \"\")\n",
    "    no_punctuation = clean_punct(no_spaces)\n",
    "    capitalize_concat_str = no_punctuation.upper()\n",
    "\n",
    "    return capitalize_concat_str\n",
    "\n",
    "# get rid of repetitions with set\n",
    "COLONIAL_PLACES = sorted(list(set(map(capitalize_and_concat, COLONIAL_PLACES_UNPROCESSED))))\n",
    "# check that contents make sense\n",
    "print(\"sanity check:\", COLONIAL_PLACES[0:100])\n",
    "\n",
    "# now we run this for the whole dataset such that we go over it in chunks of 4 words and see if we have a match\n",
    "# for this we need a hashmap version of the unprocessed list\n",
    "check_exists = sorted(list(set(map(clean_punct, map(clean_regex, COLONIAL_PLACES_UNPROCESSED)))))\n",
    "# check that contents make sense\n",
    "print(\"sanity check:\", list(check_exists)[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9dc131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 36/232695\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m json_objects:\n\u001b[32m     23\u001b[39m     new_article = {}\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     new_text = \u001b[43mkeyword_processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     new_article[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] = new_text\n\u001b[32m     26\u001b[39m     new_article[\u001b[33m\"\u001b[39m\u001b[33missue_id\u001b[39m\u001b[33m\"\u001b[39m] = article[\u001b[33m\"\u001b[39m\u001b[33missue_id\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/flashtext/keyword.py:593\u001b[39m, in \u001b[36mKeywordProcessor.replace_keywords\u001b[39m\u001b[34m(self, sentence)\u001b[39m\n\u001b[32m    591\u001b[39m idx = \u001b[32m0\u001b[39m\n\u001b[32m    592\u001b[39m sentence_len = \u001b[38;5;28mlen\u001b[39m(sentence)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m idx < sentence_len:\n\u001b[32m    594\u001b[39m     char = sentence[idx]\n\u001b[32m    595\u001b[39m     current_word += orig_sentence[idx]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# now that we have the dict, let's loop over all of the files in DATA_FOLDER / \"cleaned_json\" \n",
    "# using a window of 5 words and check for matches; if match, we run capitalize_and_concat on that string\n",
    "# and replace the 5 words with this concatenation\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "\n",
    "TOKENIZED_DATA_FOLDER = DATA_FOLDER / \"tokenized_json\"\n",
    "TOKENIZED_DATA_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "json_files = list(CLEANED_DATA_FOLDER.glob(\"*.json\"))\n",
    "\n",
    "keyword_processor = KeywordProcessor()\n",
    "for k, w in sorted(zip(check_exists, COLONIAL_PLACES)):\n",
    "    keyword_processor.add_keyword(k, w)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for i, json_file in enumerate(json_files, 1):\n",
    "        try:\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_objects = json.loads(f.read())\n",
    "            updated_json_objects = []\n",
    "\n",
    "            for article in json_objects:\n",
    "                new_article = {}\n",
    "                new_text = keyword_processor.replace_keywords(article['text'])\n",
    "                new_article[\"text\"] = new_text\n",
    "                new_article[\"issue_id\"] = article[\"issue_id\"]\n",
    "                new_article[\"article_id\"] = article[\"article_id\"]\n",
    "                updated_json_objects.append(new_article)\n",
    "\n",
    "            output_file = TOKENIZED_DATA_FOLDER / json_file.name\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(updated_json_objects, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "            print(f\"Processed file {i}/{len(json_files)}\", end=\"\\r\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {json_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d8230",
   "metadata": {},
   "source": [
    "We're further interested in commodities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93fa16fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tea', 'cotton', 'coffe', 'sugar', 'rice', 'rubber', 'petroleum', 'silk', 'tobacco', 'sisal', 'tin', 'copper', 'lead', 'zinc', 'bauxit', 'cocoa', 'oilse', 'banana', 'citrus', 'gold', 'silver', 'wool', 'timber', 'wheat', 'meat', 'peanut', 'palm-oil', 'clove', 'slave']\n"
     ]
    }
   ],
   "source": [
    "import snowballstemmer\n",
    "stemmer = snowballstemmer.stemmer('english')\n",
    "\n",
    "LIST_OF_WORDS = [stemmer.stemWord(word) for word in [\n",
    "    \"tea\", \"cotton\", \"coffee\", \"sugar\", \"rice\", \"rubber\",\n",
    "    \"petroleum\", \"silk\", \"tobacco\", \"sisal\", \"tin\", \"copper\",\n",
    "    \"lead\", \"zinc\", \"bauxite\", \"cocoa\", \"oilseeds\", \"bananas\",\n",
    "    \"citrus\", \"gold\", \"silver\", \"wool\", \"timber\", \"wheat\", \"meat\",\n",
    "    \"peanuts\", \"palm-oil\", \"cloves\", \"slave\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(LIST_OF_WORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e562d",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "We use the snowball stemmer on all the words. This allows us to detect different conjugations. This will take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076d04f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 2/232695\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m new_article = {}\n\u001b[32m     17\u001b[39m words = article[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m stemmed_words = [\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     word \u001b[38;5;28;01mif\u001b[39;00m word.isupper() \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mstemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstemWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words\n\u001b[32m     21\u001b[39m ]\n\u001b[32m     22\u001b[39m new_article[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(stemmed_words)\n\u001b[32m     23\u001b[39m new_article[\u001b[33m\"\u001b[39m\u001b[33missue_id\u001b[39m\u001b[33m\"\u001b[39m] = article[\u001b[33m\"\u001b[39m\u001b[33missue_id\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/snowballstemmer/basestemmer.py:285\u001b[39m, in \u001b[36mBaseStemmer.stemWord\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstemWord\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_current(word)\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_current()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/snowballstemmer/english_stemmer.py:580\u001b[39m, in \u001b[36mEnglishStemmer._stem\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m.cursor = \u001b[38;5;28mself\u001b[39m.limit - v_8\n\u001b[32m    579\u001b[39m v_9 = \u001b[38;5;28mself\u001b[39m.limit - \u001b[38;5;28mself\u001b[39m.cursor\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__r_Step_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[38;5;28mself\u001b[39m.cursor = \u001b[38;5;28mself\u001b[39m.limit - v_9\n\u001b[32m    582\u001b[39m v_10 = \u001b[38;5;28mself\u001b[39m.limit - \u001b[38;5;28mself\u001b[39m.cursor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/snowballstemmer/english_stemmer.py:385\u001b[39m, in \u001b[36mEnglishStemmer.__r_Step_3\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__r_Step_3\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28mself\u001b[39m.ket = \u001b[38;5;28mself\u001b[39m.cursor\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     among_var = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_among_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEnglishStemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43ma_8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m among_var == \u001b[32m0\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/snowballstemmer/basestemmer.py:187\u001b[39m, in \u001b[36mBaseStemmer.find_among_b\u001b[39m\u001b[34m(self, v)\u001b[39m\n\u001b[32m    185\u001b[39m     diff = -\u001b[32m1\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m diff = \u001b[38;5;28;43mord\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m - \u001b[38;5;28mord\u001b[39m(w.s[i2])\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m diff != \u001b[32m0\u001b[39m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "STEMMED_DATA_FOLDER = DATA_FOLDER / \"stemmed_json\"\n",
    "STEMMED_DATA_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "json_files = list(TOKENIZED_DATA_FOLDER.glob(\"*.json\"))\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for i, json_file in enumerate(json_files, 1):\n",
    "        try:\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_objects = json.loads(f.read())\n",
    "            updated_json_objects = []\n",
    "\n",
    "            for article in json_objects:\n",
    "                new_article = {}\n",
    "                words = article['text'].split(\" \")\n",
    "                stemmed_words = [\n",
    "                    word if word.isupper() else stemmer.stemWord(word)\n",
    "                    for word in words\n",
    "                ]\n",
    "                new_article[\"text\"] = \" \".join(stemmed_words)\n",
    "                new_article[\"issue_id\"] = article[\"issue_id\"]\n",
    "                new_article[\"article_id\"] = article[\"article_id\"]\n",
    "                updated_json_objects.append(new_article)\n",
    "\n",
    "            output_file = STEMMED_DATA_FOLDER / json_file.name\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(updated_json_objects, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "            print(f\"Processed file {i}/{len(json_files)}\", end=\"\\r\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {json_file}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f98a3",
   "metadata": {},
   "source": [
    "Now we will count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4b8b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON files: 232694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-32:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-17:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/miika.piiparinen/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m     53\u001b[39m mp.set_start_method(\u001b[33m'\u001b[39m\u001b[33mfork\u001b[39m\u001b[33m'\u001b[39m, force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mrun_in_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mrun_in_parallel\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_in_parallel\u001b[39m():\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Executor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, (file_result, file) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_files\u001b[49m\u001b[43m)\u001b[49m, json_files), start=\u001b[32m1\u001b[39m):\n\u001b[32m     43\u001b[39m             all_rows.update(file_result)\n\u001b[32m     44\u001b[39m             out = WORD_COUNT_FOLDER / file.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py:851\u001b[39m, in \u001b[36mProcessPoolExecutor.map\u001b[39m\u001b[34m(self, fn, timeout, chunksize, *iterables)\u001b[39m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize < \u001b[32m1\u001b[39m:\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mchunksize must be >= 1.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m results = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_process_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m                      \u001b[49m\u001b[43m_get_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:608\u001b[39m, in \u001b[36mExecutor.map\u001b[39m\u001b[34m(self, fn, timeout, chunksize, *iterables)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    606\u001b[39m     end_time = timeout + time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m fs = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*iterables)]\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[32m    611\u001b[39m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult_iterator\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py:819\u001b[39m, in \u001b[36mProcessPoolExecutor.submit\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28mself\u001b[39m._queue_count += \u001b[32m1\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;66;03m# Wake up queue management thread\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_executor_manager_thread_wakeup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwakeup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._safe_to_dynamically_spawn_children:\n\u001b[32m    822\u001b[39m     \u001b[38;5;28mself\u001b[39m._adjust_process_count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/concurrent/futures/process.py:89\u001b[39m, in \u001b[36m_ThreadWakeup.wakeup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._closed:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py:200\u001b[39m, in \u001b[36m_ConnectionBase.send_bytes\u001b[39m\u001b[34m(self, buf, offset, size)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m offset + size > n:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbuffer length < offset + size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m:\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py:427\u001b[39m, in \u001b[36mConnection._send_bytes\u001b[39m\u001b[34m(self, buf)\u001b[39m\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m._send(buf)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[32m    425\u001b[39m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[32m    426\u001b[39m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/multiprocessing/connection.py:384\u001b[39m, in \u001b[36mConnection._send\u001b[39m\u001b[34m(self, buf, write)\u001b[39m\n\u001b[32m    382\u001b[39m remaining = \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     n = \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m     remaining -= n\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m remaining == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor as Executor\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import copy\n",
    "import json\n",
    "\n",
    "WORD_COUNT_FOLDER = DATA_FOLDER / \"word_counts\"\n",
    "WORD_COUNT_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "WORDS_TO_LOOK_FOR = LIST_OF_WORDS + COLONIAL_PLACES\n",
    "WORD_COUNTS = {word: 0 for word in WORDS_TO_LOOK_FOR}\n",
    "\n",
    "def process_file(json_file: Path) -> dict:\n",
    "    results = {}\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data: list[dict] = json.load(f)\n",
    "            for article in data:\n",
    "                word_counts = copy.copy(WORD_COUNTS)\n",
    "                key = article[\"article_id\"]\n",
    "                words = article['text'].split()\n",
    "                hist = Counter(words)\n",
    "                for w in WORDS_TO_LOOK_FOR:\n",
    "                    word_counts[w] = hist[w]\n",
    "                word_counts[\"total_words\"] = len(words)\n",
    "                word_counts[\"issue_id\"] = article.get(\"issueID\", \"unknown\")\n",
    "                word_counts[\"file_name\"] = json_file.name\n",
    "                results[key] = word_counts\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {json_file}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in file {json_file}: {e}\")\n",
    "    return results\n",
    "\n",
    "json_files = list(STEMMED_DATA_FOLDER.glob(\"*.json\"))\n",
    "print(f\"Number of JSON files: {len(json_files)}\")\n",
    "\n",
    "all_rows = {}\n",
    "\n",
    "def run_in_parallel():\n",
    "    with Executor() as executor:\n",
    "        for i, (file_result, file) in enumerate(zip(executor.map(process_file, json_files), json_files), start=1):\n",
    "            all_rows.update(file_result)\n",
    "            out = WORD_COUNT_FOLDER / file.name\n",
    "            with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(file_result, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Processed file {i}/{len(json_files)}\", end=\"\\r\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing as mp\n",
    "    mp.set_start_method('fork', force=True)\n",
    "\n",
    "    run_in_parallel()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f727a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globbing...\n",
      "df created232694/232694\n"
     ]
    }
   ],
   "source": [
    "from src.settings import DATA_FOLDER\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "import orjson\n",
    "\n",
    "def calculate_tf_idf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    id_fields = [\"issue_id\", \"article_id\", \"file_name\"]\n",
    "    id_data = df[id_fields] if all(field in df.columns for field in id_fields) else None\n",
    "\n",
    "    if 'total_words' in df.columns:\n",
    "        df = df.drop(columns=[\"total_words\"])\n",
    "    df = df.drop(columns=id_fields, errors='ignore')\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf_matrix: csr_matrix = transformer.fit_transform(df.values)\n",
    "    tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidf_array, index=df.index, columns=df.columns)\n",
    "    tfidf_df[\"tf-idf\"] = tfidf_df.sum(axis=1)\n",
    "\n",
    "    if id_data is not None:\n",
    "        tfidf_df = pd.concat([tfidf_df, id_data], axis=1)\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "# load all word-count JSONs into a single DataFrame\n",
    "word_count_folder = DATA_FOLDER / \"word_counts\"\n",
    "print(\"globbing...\")\n",
    "json_file_paths = word_count_folder.glob(\"*.json\")\n",
    "dfs = []\n",
    "count = 0\n",
    "for json_file in json_file_paths:\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = orjson.loads(f.read())\n",
    "    dfs.append(pd.DataFrame.from_dict(data, orient=\"index\"))\n",
    "    count += 1\n",
    "    print(f\"processed {count}/{len(json_files)}\", end=\"\\r\")\n",
    "# save the concatenated dataframe here to DATA_FOLDER\n",
    "combined_df = pd.concat(dfs, axis=0)\n",
    "combined_df.to_csv(DATA_FOLDER / \"word_counts_combined.csv\", sep=\",\", index=True)\n",
    "\n",
    "print(\"df created\")\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df_tf_idf = calculate_tf_idf(df)\n",
    "df_tf_idf.to_csv(DATA_FOLDER / \"tf_idf.csv\", index=True, encoding='utf-8', sep=\"@\")\n",
    "\n",
    "# sort by tf-idf value and get top 100\n",
    "df_tf_idf.sort_values(by=\"tf-idf\", ascending=False, inplace=True)\n",
    "df_tf_idf = df_tf_idf.head(100)\n",
    "df_tf_idf.to_csv(DATA_FOLDER / \"tf_idf_top_100.csv\", index=True, encoding='utf-8', sep=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd6670",
   "metadata": {},
   "source": [
    "Now we use the TF-IDF dataset to fetch top N articles, and perform NER on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading df\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreading df\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_tf_idf = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_FOLDER\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtf_idf.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m@\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdf created\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m df_tf_idf.sort_values(by=\u001b[33m\"\u001b[39m\u001b[33mtf-idf\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:331\u001b[39m, in \u001b[36mgetstate\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"reading df\")\n",
    "df_tf_idf = pd.read_csv(DATA_FOLDER / \"tf_idf.csv\", delimiter=\"@\")\n",
    "print(\"df created\")\n",
    "df_tf_idf.sort_values(by=\"tf-idf\", ascending=False, inplace=True)\n",
    "df_tf_idf = df_tf_idf.head(50000)\n",
    "df_tf_idf.to_csv(DATA_FOLDER / \"tf_idf_top_50000.csv\", index=True, encoding='utf-8', sep=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b451168",
   "metadata": {},
   "source": [
    "Then, we perform network analysis to visualize the results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
